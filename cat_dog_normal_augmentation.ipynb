{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOq5wU1nDAScTWaAKmjZxUm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaykumar080286/DeepLearning/blob/master/cat_dog_normal_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "qrY5JbpdB3vV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,Flatten,MaxPool2D, Conv2D,BatchNormalization, Activation, MaxPool2D # Added Activation\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to your zip file\n",
        "zip_path = \"/content/dogs_vs_cats.zip\"\n",
        "\n",
        "# Open the zip file\n",
        "zip_ref= zipfile.ZipFile(zip_path, 'r')\n",
        "zip_ref.extractall(\"/content\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "DPnIZawgCVry"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir = \"/content/dogs_vs_cats/train\"\n",
        "category=['cats','dogs']"
      ],
      "metadata": {
        "id": "qSS_wKa2KF5I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "data = []\n",
        "base_path = \"/content/dogs_vs_cats/train\"   # your dataset root\n",
        "categories = [\"cats\", \"dogs\"]               # folder names\n",
        "\n",
        "for i, category in enumerate(categories):\n",
        "    folder_path = os.path.join(base_path, category)\n",
        "    label = i   # 0 for cats, 1 for dogs\n",
        "\n",
        "    for j in os.listdir(folder_path):\n",
        "        img_path = os.path.join(folder_path, j)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        if img is None:   # skip unreadable files\n",
        "            continue\n",
        "        img = cv2.resize(img, (150, 150))  # force same size\n",
        "        data.append([img, label])\n",
        "\n",
        "print(f\"Loaded {len(data)} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLMGVz42LjOU",
        "outputId": "1dd9f720-5603-4cff-bfe3-022ca24ce501"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 872 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(data)"
      ],
      "metadata": {
        "id": "vXMDl1m4Mo2Z"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 256\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in data:\n",
        "    X.append(i[0])\n",
        "    y.append(i[1])\n",
        "\n",
        "X = np.array(X, dtype=np.float32) / 255.0   # normalize\n",
        "y = np.array(y)\n"
      ],
      "metadata": {
        "id": "zxX59AKbNA2q"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE9BLxPiQx4u",
        "outputId": "232d6145-0479-4849-f005-cc5f5742c062"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(872, 150, 150, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdS5mUq1RVte",
        "outputId": "2cf74b64-2dcf-4300-959c-e668f973b466"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(872,)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lIH3mb-R56l"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGosvQH2TKCw",
        "outputId": "19e8486a-afa9-4013-c78e-2f46883861c3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "nh57nxx6Tfey"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs=5, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnNcxOqWUCFj",
        "outputId": "eeab992a-4718-4eee-9141-c783b7ad6897"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 907ms/step - accuracy: 0.6261 - loss: 0.6613 - val_accuracy: 0.7614 - val_loss: 0.5454\n",
            "Epoch 2/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 878ms/step - accuracy: 0.6790 - loss: 0.6662 - val_accuracy: 0.7614 - val_loss: 0.6335\n",
            "Epoch 3/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 910ms/step - accuracy: 0.7029 - loss: 0.6549 - val_accuracy: 0.7614 - val_loss: 0.6253\n",
            "Epoch 4/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 921ms/step - accuracy: 0.7150 - loss: 0.6175 - val_accuracy: 0.3977 - val_loss: 0.6944\n",
            "Epoch 5/5\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 824ms/step - accuracy: 0.6454 - loss: 0.6298 - val_accuracy: 0.7614 - val_loss: 0.5048\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7acf9623ddf0>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U5PDqjVIUN2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With Data Augmentation**"
      ],
      "metadata": {
        "id": "-8is2i3TU6uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "g7otR1-qU9tC"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=16"
      ],
      "metadata": {
        "id": "EOjD72BuVMXI"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True\n",
        "        )"
      ],
      "metadata": {
        "id": "Qu7eYFvkVWgQ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "4b-UMO4BVzaQ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_dataset.flow_from_directory(\n",
        "        '/content/dogs_vs_cats/train',  # this is the target directory\n",
        "        target_size=(150, 150),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')  # sin\n",
        "validation_generator = test_dataset.flow_from_directory(\n",
        "        '/content/dogs_vs_cats/test',\n",
        "        target_size=(150, 150),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FQ9p9RIV6z9",
        "outputId": "d242533f-1922-45a6-e5cd-8c33408bbc5d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 872 images belonging to 2 classes.\n",
            "Found 5000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(150, 150, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlFUrUMKW3qj",
        "outputId": "a4cb0d8d-0c91-43af-a305-578df5e0eda4"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "DWGxDfEYX5Cz"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=2000 // batch_size,\n",
        "        epochs=25,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=800 // batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tRVm8B8X8Aa",
        "outputId": "0693e700-8851-4946-a284-8beb0bfbef11"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m 55/125\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 510ms/step - accuracy: 0.7190 - loss: 0.6281"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 275ms/step - accuracy: 0.7126 - loss: 0.6292 - val_accuracy: 0.5000 - val_loss: 0.7413\n",
            "Epoch 2/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 330ms/step - accuracy: 0.7150 - loss: 0.6134 - val_accuracy: 0.5200 - val_loss: 0.7837\n",
            "Epoch 3/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 268ms/step - accuracy: 0.7153 - loss: 0.5840 - val_accuracy: 0.5550 - val_loss: 0.6675\n",
            "Epoch 4/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 278ms/step - accuracy: 0.7132 - loss: 0.5877 - val_accuracy: 0.5250 - val_loss: 0.6742\n",
            "Epoch 5/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 278ms/step - accuracy: 0.7167 - loss: 0.5761 - val_accuracy: 0.5138 - val_loss: 0.8574\n",
            "Epoch 6/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 272ms/step - accuracy: 0.7195 - loss: 0.5807 - val_accuracy: 0.6550 - val_loss: 0.6361\n",
            "Epoch 7/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 331ms/step - accuracy: 0.7096 - loss: 0.5466 - val_accuracy: 0.6500 - val_loss: 0.6590\n",
            "Epoch 8/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 324ms/step - accuracy: 0.7512 - loss: 0.5421 - val_accuracy: 0.6313 - val_loss: 0.6984\n",
            "Epoch 9/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 326ms/step - accuracy: 0.7299 - loss: 0.5385 - val_accuracy: 0.6587 - val_loss: 0.6546\n",
            "Epoch 10/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 276ms/step - accuracy: 0.7458 - loss: 0.5283 - val_accuracy: 0.6300 - val_loss: 0.6302\n",
            "Epoch 11/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 606ms/step - accuracy: 0.7516 - loss: 0.5246 - val_accuracy: 0.6363 - val_loss: 0.6954\n",
            "Epoch 12/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 267ms/step - accuracy: 0.7714 - loss: 0.5041 - val_accuracy: 0.6325 - val_loss: 0.7228\n",
            "Epoch 13/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 609ms/step - accuracy: 0.7754 - loss: 0.5081 - val_accuracy: 0.6425 - val_loss: 0.7765\n",
            "Epoch 14/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 320ms/step - accuracy: 0.7930 - loss: 0.4655 - val_accuracy: 0.6200 - val_loss: 0.8517\n",
            "Epoch 15/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 315ms/step - accuracy: 0.7879 - loss: 0.4736 - val_accuracy: 0.7063 - val_loss: 0.5735\n",
            "Epoch 16/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 281ms/step - accuracy: 0.8236 - loss: 0.4122 - val_accuracy: 0.7125 - val_loss: 0.5701\n",
            "Epoch 17/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 271ms/step - accuracy: 0.8134 - loss: 0.4417 - val_accuracy: 0.7113 - val_loss: 0.5797\n",
            "Epoch 18/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 275ms/step - accuracy: 0.7987 - loss: 0.4269 - val_accuracy: 0.5962 - val_loss: 0.9883\n",
            "Epoch 19/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 276ms/step - accuracy: 0.8156 - loss: 0.4212 - val_accuracy: 0.6612 - val_loss: 0.7156\n",
            "Epoch 20/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 278ms/step - accuracy: 0.8172 - loss: 0.4380 - val_accuracy: 0.6725 - val_loss: 0.7124\n",
            "Epoch 21/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 326ms/step - accuracy: 0.8328 - loss: 0.3864 - val_accuracy: 0.6712 - val_loss: 0.7183\n",
            "Epoch 22/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 276ms/step - accuracy: 0.8241 - loss: 0.3923 - val_accuracy: 0.7075 - val_loss: 0.6214\n",
            "Epoch 23/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 274ms/step - accuracy: 0.8347 - loss: 0.3960 - val_accuracy: 0.7138 - val_loss: 0.6025\n",
            "Epoch 24/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 276ms/step - accuracy: 0.8266 - loss: 0.3790 - val_accuracy: 0.7325 - val_loss: 0.5974\n",
            "Epoch 25/25\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 605ms/step - accuracy: 0.8105 - loss: 0.4027 - val_accuracy: 0.7075 - val_loss: 0.6981\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7acf75613f50>"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
      ],
      "metadata": {
        "id": "1BjhXKMvYIx-"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "apheWgBGczAv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}